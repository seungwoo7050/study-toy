# C++ 실시간 PVP 게임 서버 기초 학습 문서

## 0. 이 문서로 어디까지 가는가

최종 목표:

1. **실시간 PVP 게임 서버의 전체 구조**를 말로 설명할 수 있다.
2. **틱(Tick) 기반 게임 루프**와 **authoritative 서버** 개념을 이해한다.
3. **TCP / UDP / WebSocket**을 어디에 써야 하는지 구분해서 설명할 수 있다.
4. **세션 / 매치메이킹 / 매치 상태**를 간단한 상태도나 struct로 모델링할 수 있다.
5. 어떤 데이터를 **Postgres(영속)**에, 어떤 데이터를 **Redis(실시간 상태)**에 둘지 판단할 수 있다.

학습 대상 프로젝트 예시:

- `cpp-pvp-server` (1v1 Duel, 이후 UDP/분산까지 확장되는 게임 서버)

선행 완료 전제:

- `C++ 네트워크 프로그래밍 기초 학습 문서(cpp.md)`를 따라가서  
  **단일 TCP 에코 서버**를 직접 구현해본 상태. :contentReference[oaicite:0]{index=0}
- 리눅스/WSL에서 C++ 빌드, 간단 Make/CMake 사용 가능.
- TCP/UDP, IP/포트 정도의 네트워크 기초는 알고 있다. :contentReference[oaicite:1]{index=1}

이 문서에서 다루지 않는 것들:

- C++ 멀티스레딩/락/코루틴 상세 설계
- 실제 UDP 넷코드(재전송, 패킷 압축, 롤백, 히트 판정 세부 구현)
- Anti-cheat 구체 기술
- Kubernetes, 서비스 메시, 고급 운영 스택 설계

---

## 1. 실시간 게임 서버 전체 그림

### 1.1 싱글플레이 vs 온라인 멀티플레이

- **싱글플레이**
  - 모든 로직이 **클라이언트(로컬)**에서 돈다.
  - 세이브 파일만 잘 관리하면 됨.
- **온라인 멀티플레이**
  - **여러 클라이언트가 동시에 같은 "게임 상태"를 공유**한다.
  - 중간에 **서버**를 두고, 모든 플레이어가 그 서버를 통해 통신.

차이의 핵심은:

> "한 대에서 혼자 노는지" vs "여러 대에서 공유된 상태를 맞춰야 하는지"

멀티가 되는 순간, **시간 / 네트워크 / 일관성 / 보안**이 전부 문제로 튀어나온다.

---

### 1.2 턴제 / 실시간 / PVP 차이

- **턴제**
  - 예: 체스, 턴제 RPG
  - "입력 이벤트"만 주고받으면 됨 (턴마다 한 번씩).
  - 지연이 조금 있어도 치명적이지 않음.

- **실시간(PVE 중심)**
  - 몬스터/AI 상대, Co-op 등
  - 약간의 지연은 허용 가능. 그래도 **플레이 감각**은 중요.

- **실시간 PVP**
  - 사람이 사람을 직접 때리는 구조.
  - 지연과 판정 차이에 민감.
  - 치트(클라이언트 조작) 방지 필요.

이 문서는 **실시간 PVP**를 전제로 한다.  
그래서 "서버 기준 시간"과 "서버가 진짜 상태를 가진다"는 전제가 핵심이다.

---

### 1.3 서버 사이드 컴포넌트 개요

기본적으로 이런 컴포넌트들을 갖게 된다 (이름은 예시):

- **Gateway / Lobby 서버**
  - 로그인 / 인증
  - 로비, 매치메이킹 API
  - WebSocket 연결 관리 (로비/알림/채팅)

- **Game 서버 샤드**
  - 실제 **틱 루프**가 돌아가는 프로세스
  - 여러 매치를 동시에 처리
  - UDP/WS/TCP로 클라이언트와 직접 통신

- **저장소**
  - Postgres: 유저, 프로필, 매치 기록, 통계 등 **영속 데이터**
  - Redis: 세션, 매치 큐, 실시간 카운터 등 **빠른 상태**

- **모니터링/로깅**
  - Prometheus/Grafana, 로그 수집기
  - 틱 처리 시간, 패킷 수, 에러율 모니터링

실제 프로젝트에서는 "한 바이너리에서 다 돌리다가", 점점 분리하는 방식으로 발전시키면 된다.

---

### 1.4 1판의 라이프사이클 개요

실제 1판이 어떻게 흘러가는지, 큰 그림만 잡는다.

1. **로그인**
   - 클라이언트 → Gateway: 로그인/토큰 발급
2. **로비**
   - 현재 랭크, 통계 등 조회
3. **매치메이킹**
   - 큐에 들어감 (모드, 레이팅, 지역 등 기준)
   - 서버가 다른 플레이어와 페어링
4. **매치 생성 & 배정**
   - 새로운 `matchId` 생성
   - 어떤 Game 서버 샤드에 붙일지 결정
   - 각 세션에 "너는 이 `matchId`, 이 샤드로 접속해라" 정보 전달
5. **인게임**
   - Game 서버 틱 루프 시작
   - 클라이언트 → 서버: 입력 이벤트
   - 서버 → 클라이언트: 상태 스냅샷/이벤트
6. **종료 & 결과 저장**
   - 승/패, 점수, 레이팅 변동 계산
   - Postgres에 기록
7. **로비 복귀**
   - 세션 상태를 다시 로비 상태로 변경

이 흐름이 머릿속에 있으면, 이후 세션/매치/저장소 설계가 훨씬 정리된다.

---

### 1.5 자주 하는 실수 & 팁

- "웹 서버랑 비슷하겠지" 하고 **틱 루프 개념 없이** 요청/응답만 생각함
- 로비/인게임/매치 상태를 전부 "그냥 enum 하나"로 뭉개고 시작
- 모든 걸 **DB 하나**에만 우겨 넣으려다가, 성능과 설계 둘 다 꼬이는 경우

---

## 2. 게임 루프와 시간

### 2.1 틱(Tick) 기반 게임 루프 개념

실시간 PVP에서 서버는 보통 **고정된 주기**로 게임을 진행한다.

- 예: 초당 30틱 → `TICK_INTERVAL = 33ms`
- 매 틱마다:
  1. 네트워크로 들어온 **입력** 수집
  2. 그 틱의 입력을 적용해서 **게임 상태 업데이트**
  3. 갱신된 상태를 **클라이언트에게 전송**

간단한 형태:

```cpp
constexpr int TICKS_PER_SECOND = 30;
constexpr std::chrono::milliseconds TICK_INTERVAL(1000 / TICKS_PER_SECOND);

void run_game_loop() {
    using clock = std::chrono::steady_clock;

    bool running = true;
    auto next_tick = clock::now();

    while (running) {
        auto now = clock::now();
        if (now < next_tick) {
            std::this_thread::sleep_until(next_tick);
            continue;
        }

        const float dt = 1.0f / TICKS_PER_SECOND;

        poll_network_inputs();   // 클라이언트 입력 모으기
        update_game_world(dt);   // 위치, 스킬, 충돌 등 업데이트
        broadcast_world_state(); // 상태(또는 일부) 전송

        next_tick += TICK_INTERVAL;
    }
}
```

핵심:

* 서버 시뮬레이션의 시간 단위는 **틱**이다.
* 틱마다 "입력 → 상태 업데이트 → 상태 전송"이 깨지지 않도록 유지하는 게 중요하다.

---

### 2.2 고정 timestep vs 가변 timestep

* **고정 timestep**

  * `dt`가 항상 동일 (예: `1.0 / 30.0`)
  * 로직이 단순하고 재현성이 좋음 (디버깅, 리플레이에 유리)

* **가변 timestep**

  * `dt`를 실제 경과 시간으로 계산 (`now - last`)
  * 프레임 단위 게임에서 자주 쓰는 방식

실시간 PVP 서버는 대부분 **고정 timestep**을 선택한다.

* 판정/리플레이/테스트 시 "같은 입력이면 같은 결과"가 나와야 하기 때문.

---

### 2.3 서버 틱 레이트 vs 클라이언트 FPS

* **서버 틱 레이트**

  * 서버가 게임 상태를 업데이트하는 속도
  * 예: 20, 30, 60 TPS(Ticks Per Second)

* **클라이언트 FPS**

  * 클라이언트(게임 클라이언트)가 화면을 그리는 속도
  * 예: 60, 120 FPS

두 값은 완전히 별개다.

* 서버는 30틱, 클라이언트는 60FPS일 수 있다.
* 클라이언트는 서버에서 받은 상태를 **보간(interpolation)**해서 매 프레임 그리면 된다.

---

### 2.4 틱 루프에서의 처리 순서

일반적인 순서는:

1. **입력 수집**

   * 소켓/버퍼에서 아직 처리 안 한 입력 패킷을 모음
   * 각 플레이어별 `pendingInputs`에 쌓는다.

2. **명령 처리**

   * 이번 틱에 실제로 적용할 입력(커맨드)만 꺼냄
   * "틱 번호"를 함께 관리하면 나중에 예측/보정에 쓸 수 있음

3. **게임 월드 업데이트**

   * 위치/속도/상태/쿨다운/프로젝트일 등 업데이트
   * 충돌/피격/죽음 판정 등

4. **상태 스냅샷 준비**

   * 모든 객체를 다 보내는 대신, 필요한 정보만 뽑아 내기
   * 예: 플레이어 위치, HP, 주요 이벤트(킬, 히트) 등

5. **브로드캐스트**

   * 각 플레이어 연결로 스냅샷/이벤트 전송

---

### 2.5 실습 1: 고정 틱 루프 스켈레톤 만들기

목표:

* 실제 게임 로직 없이, **로그만 찍는 틱 루프**를 C++로 한 번 구현해 본다.

가이드:

1. `GameLoop` 클래스를 만든다.

   * `start()` / `stop()` 메서드
   * 내부에서 while 루프 + `std::chrono::steady_clock` 사용

2. 초당 20틱으로 설정하고:

   * 틱 번호, 현재 시간, `dt`(고정값)를 로그로 출력

3. 일정 시간(예: 5초) 후 `stop()`을 호출해서 종료

이 정도를 직접 구현해 보면, 이후 틱 루프 안에 어떤 로직이 들어갈지 감이 잡힌다.

---

### 2.6 자주 하는 실수 & 팁

* `sleep`만 믿고 루프를 돌리다가 **시간 누적 오차**를 무시

  * 항상 "목표 시각(next_tick)" 기준으로 `sleep_until` 하는 패턴 유지
* 틱 루프와 네트워크 수신을 한 스레드에서 blocking하게 처리

  * 이후 실제 구현에서는 **논블로킹 I/O + 별도 스레드/이벤트 루프**를 고려해야 한다.

---

## 3. Authoritative 서버와 클라이언트 예측

### 3.1 Authoritative 서버 모델 정의

Authoritative 서버:

> "게임의 진짜 상태와 판정을 서버가 모두 책임지는 모델"

* 클라이언트:

  * **입력**만 서버에 보냄 (이동, 공격, 스킬 사용 등)
* 서버:

  * 입력을 받고, 틱 루프에서 상태를 계산
  * 결과(위치, HP, 판정)를 클라이언트에게 브로드캐스트

이렇게 해야:

* 클라이언트가 메모리/패킷을 조작해도, 서버가 **최종 상태를 덮어쓴다**.
* 동기화 기준이 항상 "서버 기준"이 된다.

---

### 3.2 클라이언트가 할 수 있는 일 / 하면 안 되는 일

**가능한 일(권장)**

* 입력 발생 시점 처리

  * 키보드/마우스 입력을 로컬에 먼저 반영 (예측)
* UI, 이펙트

  * 이펙트, 사운드, 애니메이션 재생
* 과거 상태 보여주기 (보간용)

**하면 안 되는 일**

* 자기 위치/속도를 마음대로 셋팅하고 서버에 통보
* 적 HP를 직접 깎아서 서버에 "이제 HP=0"이라고 알림
* 스킬 쿨다운/범위/판정을 클라이언트가 최종 결정

클라이언트는 **"내가 이렇게 눌렀다"**만 말하고,
"이렇게 됐다"는 반드시 서버가 말해준다.

---

### 3.3 클라이언트 예측의 기본 아이디어

레이턴시 때문에, 서버 응답만 기다리면 조작감이 끊긴다.

간단한 패턴:

1. 클라이언트는 입력 발생 시:

   * **로컬에 바로 적용**해서 캐릭터를 움직여 보인다. (예측)
   * 동시에 그 입력을 서버에 전송한다. (틱 번호/시각 포함)

2. 서버는 틱 루프에서:

   * 입력 큐에서 해당 입력을 처리하고, 결과 위치/상태를 계산한다.

3. 서버는 스냅샷을 클라이언트에 전송한다.

4. 클라이언트는:

   * 자신이 예측한 상태와 **서버가 준 상태**를 비교하고,
   * 차이가 크면 순간이동시키거나, 부드럽게 보정한다.

---

### 3.4 서버 보정(서버 스냅샷 적용) 전략 개요

간단한 보정 방식:

* **하드 스냅**

  * 차이가 크면 서버 위치로 바로 덮어쓴다.
  * 순간이동 느낌이 날 수 있음.

* **소프트 보정**

  * 여러 프레임에 걸쳐 서버 위치 쪽으로 선형 보간
  * 예: 3~5프레임에 걸쳐 점점 위치를 맞춰감

실제로는:

* 자신의 캐릭터는 소프트 보정 비율을 낮게
* 다른 캐릭터는 다소 하드하게 맞춰도 괜찮은 등
  UX 관점 튜닝이 들어가지만, 이 문서에서는 개념만 알면 충분하다.

---

### 3.5 입력 기반 설계 vs 상태 기반 설계 비교

* **입력 기반**

  * 클라이언트 → 서버: "앞으로 이동", "점프", "공격" 같은 입력
  * 서버: 입력을 적용해서 상태를 계산
  * 장점: 판정/로그/리플레이에 유리
  * 단점: 서버 로직이 복잡해짐

* **상태 기반**

  * 클라이언트 → 서버: "지금 내 위치는 (x,y)야" 같은 상태
  * 서버: 검증만 하거나, 그대로 받아들이기도 함
  * 장점: 구현이 단순
  * 단점: 치트/동기화 문제 위험

실시간 PVP에서 권장하는 쪽은 **입력 기반 + 서버 authoritative 상태**다.

---

### 3.6 실습 2: 입력 큐 + 상태 업데이트 설계 스케치

목표:

* C++ struct/클래스로 입력/상태를 한 번 모델링해 본다.

예시:

* `struct InputCommand { playerId, tick, moveX, moveY, fire, skillId }`
* `struct PlayerState { playerId, position, velocity, hp, ... }`

실습 아이디어:

1. `InputCommand` 구조체 정의
2. `std::vector<InputCommand>` 또는 `std::queue<InputCommand>`로 **입력 큐**를 정의
3. "한 틱에서 어떤 순서로 InputCommand를 소비할지" 주석으로라도 써 본다.

---

### 3.7 FAQ

* Q: **"예측 없이 서버 응답만 쓰면 안 되나요?"**
  A: 기술적으로는 가능하지만, 50~100ms만 넘어가도 조작감이 눈에 띄게 둔해진다. PVP 기준으로는 거의 안 쓰는 선택지다.

* Q: **"모든 판정을 서버에서만 하면 너무 느리지 않나요?"**
  A: 판정은 서버에서 하지만, 시각 효과는 클라이언트에서 먼저 보여줄 수 있다. "판정 = 서버, 연출 = 클라이언트"로 나누면 된다.

---

## 4. 네트워크 프로토콜 전략 (TCP / UDP / WebSocket)

### 4.1 TCP 특성 요약 및 장단점

* 연결 지향, 신뢰성 보장 (순서/재전송)
* 스트림 기반 (바이트 스트림, 메시지 경계 없음)

장점:

* 전달/순서 보장이 필요할 때 안전하다.
* 구현이 상대적으로 단순하다.

단점:

* 패킷 하나 지연되면 뒤 패킷도 같이 밀림 (head-of-line blocking)
* 재전송 때문에 레이턴시 스파이크가 생길 수 있다.

---

### 4.2 UDP 특성 요약 및 장단점

* 비연결, 데이터그램(메시지) 단위 전송
* 순서/도착 보장 없음

장점:

* 오버헤드가 작고 빠르다.
* 이전 패킷을 굳이 기다릴 필요 없이, **"최신 상태만 중요"**한 경우에 적합.

단점:

* 신뢰성, 재전송, 흐름 제어를 직접 설계해야 한다.

---

### 4.3 WebSocket 개요

* HTTP 업그레이드를 통해 맺는 **TCP 기반 양방향 스트림**.
* 장점:

  * 브라우저와 궁합이 좋다.
  * 기존 HTTP 인프라(프록시, 로드밸런서)와 같이 쓰기 편함.
* 단점:

  * 결국 TCP라서 **신뢰성/재전송 비용**을 그대로 가진다.

로비/매치/채팅처럼 극단적인 레이턴시를 요구하지 않는 영역에는 잘 맞는다.

---

### 4.4 로비 / 매치 / 인게임 트래픽 분리 패턴

일반적인 조합 예:

* **HTTP/REST**

  * 로그인, 회원가입, 프로필 조회, 매치 기록 조회 등
* **WebSocket(TCP)**

  * 로비, 매치 큐 상태, 알림/채팅
* **UDP**

  * 인게임 위치/상태 스냅샷, 입력/피격 이벤트 등

첫 버전에서는 "모두 WebSocket(TCP)로 통일"해도 괜찮다.
UDP로 분리하는 건 **TPS/유저 수가 올라가고, 지연에 예민해질 때** 고려해도 늦지 않다.

---

### 4.5 메시지/패킷 설계 단위

패킷 설계 시 보통 포함되는 것:

* 타입/명령 (`messageType` or `opcode`)
* 식별자 (`playerId`, `matchId`)
* 시퀀스 번호 또는 틱 번호
* 페이로드(좌표, 상태, 스킬 정보 등)

예시(개념):

```txt
[messageType][matchId][playerId][tick][payload...]
```

JSON으로 시작해도 되지만,
TPS/유저 수가 늘면 **바이너리 포맷**을 고려해야 한다.

---

### 4.6 실습 3: TCP vs UDP 왕복 지연 측정 POC 설계

구현까지는 아니더라도, 설계를 한 번 적어 본다.

* TCP:

  * 클라이언트: "PING + 타임스탬프" 전송
  * 서버: 그대로 되돌려 보내기
  * 클라이언트: 받은 시점에서 왕복 시간 계산

* UDP:

  * 동일한 구조로, UDP 패킷으로 전송

비교 포인트:

* 패킷 손실 시 TCP/UDP가 어떻게 다른지
* 패킷 순서가 뒤집혔을 때 처리 방식

---

### 4.7 자주 하는 실수 & 안티패턴

* **모든 트래픽을 TCP + JSON 하나로만 처리**

  * 구현은 쉬운데, 레이턴시/대역폭 문제가 금방 드러난다.
* UDP를 "신뢰성이 없는 TCP" 정도로만 이해하고 설계

  * UDP는 "빠르게 최신 상태를 뿌리는" 용도에 맞게 설계해야 한다.
* 패킷 사이즈/빈도를 설계 없이 막 던짐

  * 특히 모바일/저속 네트워크에서는 바로 문제가 난다.

---

## 5. 세션 / 매치메이킹 / 매치 상태

### 5.1 플레이어 세션 상태 머신

기본 상태 흐름 예시:

1. `Disconnected`
2. `Connected` (소켓만 연결, 미인증)
3. `Authenticated` (로그인 완료)
4. `InLobby`
5. `Matching`
6. `InMatch`
7. `PostMatch` (결과 처리)
8. 다시 `InLobby` 또는 `Disconnected`

이걸 C++ enum으로 표현하면:

```cpp
enum class SessionState {
    Disconnected,
    Connected,
    Authenticated,
    InLobby,
    Matching,
    InMatch,
    PostMatch,
};
```

각 상태 전이를 **명시적으로 관리**해야 한다.
"지금 이 유저가 어디에 있는가?"를 언제든 빠르게 확인할 수 있어야 한다.

---

### 5.2 세션 식별/키 설계

주로 사용하는 식별자:

* `playerId` (DB 상 유저 PK)
* `sessionId` (로그인 세션 단위)
* `connectionId` or 소켓 FD (현재 연결에 해당하는 식별자)
* `matchId` (현재 매치)

간단한 `Session` 예시:

```cpp
struct Session {
    std::string sessionId;
    int64_t     playerId;
    SessionState state;
    std::optional<int64_t> currentMatchId;
    // 마지막 ping 시각, region, mmr 등 추가 가능
};
```

이걸 메모리 + Redis에 같이 둬서,
Gateway든 Game 서버든 **동일한 세션 정보**를 조회할 수 있게 만드는 패턴을 많이 사용한다.

---

### 5.3 매치메이킹 큐 모델

큐에 들어가는 정보:

* `playerId`
* `mmr` 또는 레이팅
* `region` / `queueType`
* `enqueueTime`

구현 예(개념):

* Redis Sorted Set:

  * key: `mm:queue:1v1`
  * score: `mmr` 또는 `(mmr + 대기시간 보정)`
  * value: `playerId` 또는 `{playerId, sessionId}` 직렬화

매치메이킹 로직:

1. 큐에서 특정 범위의 플레이어들을 읽고
2. 조건(MMR 차이, 대기시간 등)에 맞게 페어링
3. 페어링된 플레이어들로 `match` 생성

---

### 5.4 매치 객체 모델

매치 하나는 보통 이런 데이터가 필요하다:

```cpp
enum class MatchState {
    Waiting,
    Starting,
    InProgress,
    Finished,
};

struct Match {
    int64_t matchId;
    std::vector<int64_t> playerIds;
    MatchState state;
    int64_t gameServerId;   // 어느 샤드에 붙었는지
    // 시작/종료 시각, 모드, 맵, 결과 등 추가 가능
};
```

세션과의 관계:

* `Session.currentMatchId`와 `Match.playerIds`가 서로 연결
* 매치가 끝나면:

  * `Match.state = Finished`
  * 각 세션의 `state`를 `PostMatch → InLobby`로 이동

---

### 5.5 게임 상태 동기화 패턴

두 가지를 섞어서 쓰는 경우가 많다.

1. **입력 이벤트 기반**

   * 클라이언트 → 서버: "A가 공격 버튼을 눌렀다(틱 N)"
   * 서버는 그 이벤트를 처리하고 결과를 만든다.

2. **스냅샷 기반**

   * 서버 → 클라이언트: "틱 N에서 월드 상태는 이렇다"
     (플레이어 위치, HP, 주요 오브젝트 상태 등)

조합:

* 클라이언트 → 서버: **입력 위주**
* 서버 → 클라이언트: **스냅샷 + 일부 이벤트(피해, 사망 등)**

이 문서에서는:

* "서버가 authoritative 상태를 유지하고, 틱마다 최신 상태를 뿌린다"
* "클라이언트는 입력을 보내고, 서버 상태에 맞춰 화면을 그린다"

수준까지 이해하면 충분하다.

---

### 5.6 실습 4: enum + struct로 상태 모델링

간단한 문서/코드 실습:

1. `SessionState`, `MatchState` enum 정의
2. `Session`, `Match` struct 정의
3. "로그인 → 로비 → 매칭 → 인게임 → 결과 → 로비" 흐름을
   **상태 전이 다이어그램**으로 그려본다 (텍스트로 써도 됨).

이걸 한 번 그려보면, 구현할 때 "어디에서 상태가 꼬일 수 있는지"가 눈에 보인다.

---

### 5.7 FAQ

* Q: **"큐/매치를 꼭 Redis에 둬야 하나요?"**
  A: 필수는 아니지만, 여러 서버 인스턴스에서 공유해야 하고, 빠른 정렬/조회가 필요하다. 그래서 메모리 + Redis 조합이 현실적인 선택이다.

* Q: **"매치 도중에 나갔다가 재접속하면 어떻게 처리하나요?"**
  A: 보통 `Session`에 "현재 matchId + 재접속 토큰" 비슷한 정보를 두고, 일정 시간 안에 같은 매치로 다시 붙게 한다. 이 문서는 개념까지만 다루고, 구체 구현은 프로젝트 단에서 설계하면 된다.

---

## 6. 저장소 설계 – Postgres + Redis

### 6.1 영속 데이터 vs 실시간 상태 구분

먼저 이 둘을 나눈다:

* **영속 데이터(persistent)**

  * 서비스가 꺼졌다 켜져도 반드시 유지돼야 하는 것
  * 예: 계정, 인벤토리, 매치 기록, 통계

* **실시간 상태(ephemeral)**

  * 순간적인 접속 상태, 큐, 임시 카운터 등
  * 장애 시 일부 잃더라도 치명적이지 않은 것

일반적인 기준:

* "잃으면 큰일 나는 것" → Postgres (또는 다른 RDB)
* "실시간이고, 잃어도 복구 가능하거나 영향이 크지 않은 것" → Redis

---

### 6.2 Postgres에 넣을 데이터

예시:

* `users`

  * id, 닉네임, 계정 상태, 생성/갱신 시각
* `player_profiles`

  * MMR/랭크, 누적 승/패, 통계
* `matches`

  * matchId, 참여자, 모드, 시작/종료 시각, 결과
* `match_events` (선택)

  * 주요 이벤트 로그 (킬, 오브젝트 파괴 등)

특징:

* 트랜잭션, 일관성, 쿼리 유연성이 필요할 때.
* 매 틱마다 쓰면 안 되고, **매치 단위 / 이벤트 단위**로 묶어서 쓰는 게 기본이다.

---

### 6.3 Redis에 넣을 데이터

예시:

* 세션

  * `session:{sessionId} → playerId, state, lastSeen, currentMatchId`
* 매치 큐

  * `mm:queue:1v1` (sorted set)
* 활성 매치 상태(간략)

  * `match:{matchId} → shardId, state`
* 실시간 카운터

  * `online:players`, `online:matches` 같은 숫자
* 일시적인 토큰/코드

  * 재접속 토큰, 일회성 인증 코드 등

Redis는 **읽기/쓰기 빠른 key-value/자료구조 저장소**로 보면 된다.
반대로, 여기에만 영구 데이터를 두고 RDB를 안 쓰는 건 거의 추천되지 않는다.

---

### 6.4 쓰기 전략

패턴 예시:

* **매치 종료 시점 기록**

  * `Match.state`가 `Finished`로 바뀔 때:

    * Postgres에 `matches`, `player_profiles` 업데이트
    * Redis에서 관련 키 삭제/정리

* **주요 이벤트 발생 시 기록**

  * 랭크 승급, 아이템 획득, 블록/밴 같은 이벤트는 바로 DB에 기록

* **정기 배치/집계**

  * 시간대별 통계는 배치 작업으로 따로 집계해서 테이블에 저장

중요한 건:

> "실시간 상태를 전부 DB에 던지지 말고, 단위(매치/이벤트/배치)를 정해서 쓰기"

---

### 6.5 실습 5: Redis 키 설계 표 만들기

간단한 테이블을 직접 작성해본다.

예시:

| 용도      | Key 예시                | 자료구조        | TTL | 설명            |
| ------- | --------------------- | ----------- | --- | ------------- |
| 세션      | `session:{sessionId}` | Hash        | O   | 로그인 세션 정보     |
| 매치 큐    | `mm:queue:1v1`        | Sorted Set  | X   | 1v1 매치 대기열    |
| 활성 매치   | `match:{matchId}`     | Hash        | O   | 매치 → 샤드 매핑 정보 |
| 접속자 카운터 | `online:players`      | String(Int) | X   | 현재 온라인 플레이어 수 |

설계하면서 "TTL이 있어야 하는지", "재시작 시 어떤 정보는 버려도 되는지"를 같이 생각해 보면 좋다.

---

### 6.6 자주 하는 실수 & 안티패턴

* **매 틱마다 DB에 쓰기**

  * "서버 상태를 전부 DB에 넣으면 안전하지 않을까?" → 곧바로 병목/비용 지옥으로 간다.
* Redis를 **영속 저장소로 오해**

  * Redis는 메모리 기반이다. 스냅샷/로그 설정으로 복구는 가능해도, "절대 안 잃는 저장소"로 보는 건 위험하다.
* Redis 키 네이밍/TTL 전략 없이 무작정 추가

  * 나중에 "어떤 키가 뭔지" 자체가 모니터링/운영의 발목을 잡는다.

---

## 7. 분산 서버와 샤딩

> 여기부터는 "한 프로세스 안에 모든 게임을 다 넣지 않는다"는 전제를 깔고 본다.  
> 실제 구현은 프로젝트 단계에서 선택이고, 이 섹션은 개념/설계 감 잡기용이다.

### 7.1 단일 서버 구조의 한계

처음에는 보통 이렇게 시작한다:

- 하나의 바이너리 / 프로세스
  - 로그인 + 로비 + 매치메이킹 + 인게임 틱 루프 전부 처리
- 장점:
  - 구현 단순, 디버깅 쉬움
- 한계:
  - CPU, 메모리 한계에 금방 닿음
  - 프로세스 하나 죽으면 **전체 서비스 다운**
  - 배포/롤백도 한 덩어리로 묶여 있음

실시간 PVP 기준으로 **동시 매치 수/플레이어 수가 올라갈수록**:

- 틱 루프가 처리해야 할 오브젝트 수 증가
- GC/메모리/락 경합 등으로 틱 시간 벌어짐
- 한 매치의 부하가 다른 매치에 영향을 준다

그래서:

> "매치/룸/채널" 단위를 쪼개서 여러 프로세스/서버에 나누는 구조(샤딩)가 필요해진다.

---

### 7.2 샤딩 단위 선택 (매치 / 룸 / 채널)

샤딩의 기준을 어디에 둘지 먼저 정해야 한다.

대표 패턴:

1. **매치 단위 샤딩**
   - 한 프로세스(또는 스레드)가 여러 매치를 처리
   - 매치는 서로 독립, 다른 프로세스 간 공유 없음
   - PVP 1v1/소규모 매치엔 이게 일반적

2. **룸 / 채널 단위 샤딩**
   - MMO 스타일 채널, 로비, 필드 등
   - 하나의 "월드/채널"이 샤드가 됨
   - 각 샤드는 큰 월드 하나를 담당

3. **기타**
   - 지역(Region) 단위로 나누고, 그 안에서 다시 매치/룸 샤딩

이 문서에서 타깃으로 하는 건:

- `matchId`를 샤딩 단위로 보고,
- `GameServerShard` 하나가 여러 매치를 동시에 처리하는 구조.

---

### 7.3 Gateway / Lobby / Game 샤드 역할 분리

역할을 명확히 나누면, 장애/스케일링 단위도 자연스럽게 나뉜다.

- **Gateway/Lobby 서버**
  - 인증/로그인
  - 로비/매치메이킹
  - 클라이언트와의 WebSocket/HTTP 연결
  - "어느 샤드로 보내야 하는지" 라우팅 결정

- **Game 서버 샤드**
  - 틱 루프 실행
  - 매치/룸 상태 관리
  - 클라이언트(또는 Gateway)를 통한 실시간 패킷 처리

- **공통 스토리지/메시지 레이어**
  - Redis: 세션, 큐, 샤드 상태
  - DB: 영속 데이터
  - (있다면) 메시지 브로커: 샤드 간 이벤트 전달

핵심은:

> "Gateway는 얇은 디스패처, Game 샤드는 무거운 시뮬레이터"  
> 로 역할을 나누는 것이다.

---

### 7.4 matchId → shard 라우팅 전략

어떤 매치가 어느 샤드에 붙어 있는지 알아야 한다.

가장 단순한 방법:

- 매치 생성 시:
  1. 현재 샤드들의 부하 상태 확인
  2. 제일 여유 있는 샤드 하나 선택 (예: `shard-3`)
  3. `matchId → shardId` 매핑을 Redis에 기록

예시(개념):

- Redis Hash: `match:shard`
  - field: `match:{matchId}`
  - value: `shard-3`

또는:

- Key: `match:{matchId}:shard`
- Value: `shard-3`

클라이언트 라우팅 흐름 예:

1. 클라이언트가 Gateway에 "이 매치에 입장하고 싶다" 요청.
2. Gateway:
   - `matchId`로 Redis 조회 → `shardId` 획득
   - `shardId`에 해당하는 주소/포트/토큰 전달
3. 클라이언트:
   - 해당 Game 샤드에 WebSocket/UDP로 직접 연결

중요한 점:

- **"matchId → shardId" 매핑은 중앙(예: Redis)에 있다**  
  → Gateway를 여러 대 띄워도 일관된 결과를 얻을 수 있다.

---

### 7.5 로드밸런싱 기준

HTTP 서버는 QPS 기반으로 로드밸런싱하는 경우가 많지만,  
게임 서버는 **"동시 매치/플레이어 수 + 틱 처리 시간"**이 더 중요하다.

샤드 메트릭 예:

- `activeMatches`
- `activePlayers`
- `avgTickTimeMs` (최근 N초 평균)
- `cpuUsage`, `memoryUsage` (있으면 좋음)

매치 생성 시:

- 모든 샤드의 메트릭을 읽고,
- 간단한 스코어 함수로 "여유도 점수" 계산:

  ```text
  score = w1 * activeMatches + w2 * activePlayers + w3 * avgTickTimeMs
  ```

* 가장 score가 낮은(덜 바쁜) 샤드를 선택.

처음에는 대충 "동시 매치 수" 하나만 보고 분배해도 충분하다.

---

### 7.6 장애/스케일링 시나리오

개념 수준에서만 짚고 간다.

1. **샤드 다운**

   * 해당 샤드에서 돌던 매치들은 **즉시 종료 or 몰수 패 처리**가 현실적
   * Redis의 `shard:{id}:status` 같은 키로 헬스 체크
   * 다운 감지 → 해당 샤드 매치들을 "실패/패배" 처리하고 보상/페널티 정책 적용

2. **샤드 추가**

   * 새 샤드가 뜨면 `shard-list`에 등록
   * 이후 생성되는 매치부터 새 샤드에도 분배
   * 기존 매치를 "실시간으로 옮기는 것"은 상당히 복잡하니,
     처음에는 **"새 매치부터 새 샤드에만 배치"** 정도로 끝내는 게 안전하다.

3. **샤드 점검/롤링 업데이트**

   * 더 이상 새로운 매치를 배치하지 않도록 **드레인(drain) 플래그** 설정
   * 기존 매치들이 끝날 때까지 기다렸다가 프로세스 내린다
   * 롤링으로 하나씩 갈아끼우는 패턴

이 정도 그림만 잡혀 있어도,
나중에 실제 프로젝트 설계할 때 "어디까지 욕심을 낼지"를 결정하기 쉽다.

---

### 7.7 실습 6: 샤딩 테이블 설계

간단한 문서/표로만 해도 충분한 실습:

1. "샤드 상태 테이블"을 설계해본다.

예시:

| 필드              | 타입         | 설명                    |
| --------------- | ---------- | --------------------- |
| shardId         | string/int | 샤드 식별자                |
| host            | string     | 주소                    |
| port            | int        | 포트                    |
| activeMatches   | int        | 현재 매치 수               |
| activePlayers   | int        | 현재 플레이어 수             |
| avgTickTimeMs   | float      | 최근 틱 평균 시간            |
| status          | enum       | Healthy/Draining/Down |
| lastHeartbeatAt | timestamp  | 마지막 헬스 체크 시간          |

2. "matchId → shardId 매핑 테이블"도 설계해본다.

| 필드        | 타입         | 설명           |
| --------- | ---------- | ------------ |
| matchId   | int64      | 매치 ID        |
| shardId   | string/int | 매치가 붙은 샤드 ID |
| createdAt | timestamp  | 매치 생성 시각     |

이걸 실제 DB/Redis에 어떻게 맵핑할지까지 고민해보면 더 좋다.

---

## 8. 관측성(Observability) 기본

> "잘 돌아가고 있는지 / 언제 터지는지 / 왜 느려지는지"를
> 모니터링하기 위한 최소 개념 정리.

### 8.1 필요한 메트릭 정의

PVP 서버에서 특히 중요한 것들:

* **게임 루프**

  * `tick_duration_ms` (각 틱 처리 시간)
  * `tick_overrun_count` (목표 시간 초과 횟수)
* **플레이어/매치**

  * `active_players`
  * `active_matches`
  * 모드별/지역별 매치 수
* **네트워크**

  * 초당 받은/보낸 패킷 수
  * 디코딩 실패/버려진 패킷 수
* **에러**

  * 예외 발생 수
  * 주요 오류 코드별 카운트 (인증 실패, 매치 생성 실패 등)

이 정도면 Grafana 같은 대시보드에 올렸을 때,
"틱이 말라간다 / 유저가 몰린다 / 에러 터진다"를 눈으로 볼 수 있다.

---

### 8.2 로그 전략

메트릭만으로는 "무슨 일이 있었는지"까지는 알기 어렵다.
로그를 최소한 이런 기준으로 찍어두면 좋다.

* **Request 단위 상관 ID**

  * HTTP/WebSocket/UDP 상관 ID를 정해두고,
  * 같은 요청/이벤트 흐름에서 동일한 ID를 로그에 같이 찍는다.

* **Match/Player 중심 필드**

  * `matchId`
  * `playerId`
  * `shardId`
  * 이 세 개만 모든 로그에 꾸준히 넣어도
    나중에 "이 플레이어가 이 매치에서 무슨 일이 있었는지" 추적이 가능하다.

* **레벨 분리**

  * INFO: 정상 흐름(매치 생성, 시작, 종료)
  * WARN: 비정상 흐름이지만 복구 가능 (재시도, 타임아웃)
  * ERROR: 실제 실패/데이터 손상 가능성

---

### 8.3 대시보드 구성 항목

대략 이런 패널들이 있으면 유용하다.

1. **실시간 상태**

   * 현재 활성 플레이어/매치 수
   * 샤드별 activeMatches/activePlayers

2. **틱 루프 그래프**

   * `tick_duration_ms`의 p50/p95/p99
   * 1~5분 간격으로 봐도 좋고, 초단위로 봐도 좋음

3. **에러율**

   * 분당/초당 ERROR 로그 수
   * 특정 에러 코드 비율

4. **매치 관련**

   * 시간대별 매치 생성/종료 개수
   * 평균 매치 길이

처음에는 **"틱 시간 + 활성 매치/플레이어 + 에러율"** 3개만 있어도
이상 징후는 대부분 눈에 보인다.

---

### 8.4 알람 조건 예시

알람은 **너무 민감하게 잡으면 노이즈**,
너무 느슨하면 "이미 장애 난 뒤에만 울림".

예시:

* `tick_duration_ms_p95 > 2 * target_tick_ms`
  (예: 33ms 틱에서 p95가 70ms 이상이 1분 이상 지속)
* `error_rate > 평소의 3배 이상`
  (5분 이동 평균 기준)
* `activeMatches`가 평소보다 급락 (특정 샤드만)

  * 해당 샤드 다운 가능성
* 특정 에러 코드 (예: DB 연결 실패)가 짧은 시간에 집중 발생

이 정도를 기준으로 알람을 설정해두면:

* "게임은 되고 있는데 느려지기 시작하는 상황"
* "특정 샤드만 죽어 있는 상황"

을 비교적 빨리 발견할 수 있다.

---

## 9. 자주 하는 실수 & 안티패턴 모음

### 9.1 설계 관점 안티패턴

* **모든 걸 DB 하나에 몰아넣기**

  * 세션, 큐, 매치 상태, 실시간 카운터까지 전부 RDB에 저장
  * 지연/락/컨텐션으로 틱 루프가 제 시간에 돌지 못함.

* **상태/문제를 시간 개념 없이 바라보기**

  * 틱 개념 없이 "요청/응답"만 중심에 두는 설계
  * 나중에 스킬 판정/동기화 문제 설명이 안 됨.

* **"먼저 기능만 만들고, 나중에 실시간/성능은 붙이면 되겠지"**

  * 실시간 특성(틱, 프로토콜, 상태 동기화) 자체가
    설계 초반부터 들어가 있어야 한다.

---

### 9.2 구현 관점 안티패턴

* **블로킹 I/O를 틱 루프 안에서 직접 호출**

  * DB 쿼리, 파일 IO, HTTP 요청 등을 틱 루프 안에서 기다림
  * 한 유저의 문제가 전체 틱 루프를 묶어버리는 구조가 된다.

* **예외/에러 처리 미비**

  * 네트워크 디코딩 실패, 잘못된 패킷, 시간 초과 등에서
    그냥 크래시 나거나 상태가 꼬이게 놔둠.

* **패킷 사이즈/빈도 제한 없음**

  * 클라이언트가 과도한 패킷을 보내도 그대로 처리
  * DoS/스팸/악의적 클라이언트에 취약

---

### 9.3 운영 관점 안티패턴

* **모니터링 없이 "느낌"만으로 운영**

  * "요즘 서버 좀 느린 거 같은데…" 수준에서만 파악
  * 숫자/그래프가 없어서 원인 분석이 안 됨.

* **샤드/매치/세션 상태 정리가 없음**

  * 프로세스 죽으면 Redis/DB에 쓰레기 상태가 남아도 정리 안 함.
  * 재접속/재시작 시 애매한 상태에서 버그 파티.

* **버전/태그 관리를 안 함**

  * 어떤 빌드/커밋이 현재 서버에 돌아가는지 모르는 상태로 배포

---

## 10. FAQ / 자기점검

### 10.1 개념 체크 질문 리스트

아래 질문에 **대답을 글로라도 정리해보면**,
이 문서에서 말하고자 한 핵심을 대부분 소화한 상태라고 보면 된다.

1. **틱 루프**

   * "서버 틱 루프가 하는 일을 순서대로 설명해 보라."
   * "고정 timestep을 쓰는 이유는 무엇인가?"

2. **Authoritative 서버 / 예측**

   * "Authoritative 서버 모델이 무엇인지 설명해 보라."
   * "클라이언트 예측과 서버 보정이 왜 필요한가?"

3. **프로토콜**

   * "TCP / UDP / WebSocket 각각 어디에 쓰는 게 적절한지 예를 들어 설명해 보라."
   * "왜 인게임 실시간 상태 동기화에 UDP를 고려하는가?"

4. **세션 / 매치**

   * "SessionState와 MatchState를 enum으로 정의해 보고, 상태 전이 흐름을 설명해 보라."
   * "매치메이킹 큐와 매치 객체의 역할 차이는 무엇인가?"

5. **저장소 / 샤딩 / 모니터링**

   * "어떤 데이터는 Postgres에, 어떤 데이터는 Redis에 두는 게 좋은지 기준을 설명해 보라."
   * "matchId → shardId 매핑이 왜 필요한가?"
   * "틱 루프 관련해서 어떤 메트릭을 모니터링해야 하는가?"

---

### 10.2 프로젝트 진입 전 체크리스트

실제 `cpp-pvp-server` 같은 프로젝트를 시작하기 전에,
아래 항목에 스스로 체크를 해본다.

* [ ] **틱 루프**를 C++로 직접 구현해 봤다. (로그만 찍어도 됨)
* [ ] `SessionState`, `MatchState`, `Session`, `Match` 구조를 스스로 설계/작성해 봤다.
* [ ] "로비/매치/인게임" 트래픽을 **HTTP / WebSocket / UDP** 중 어떤 걸 쓸지, 이유와 함께 설명할 수 있다.
* [ ] "Postgres에만 두면 안 되는 데이터"와 "Redis에만 두면 위험한 데이터"가 무엇인지 예시를 들 수 있다.
* [ ] 샤드/매치/세션/플레이어 관련 **기본 메트릭/로그 필드 목록**을 문서로 정리해 봤다.

이 체크리스트가 대부분 "예"라면,
이제 실제 PVP 서버 레포의 `design/` 문서와 코드를 보면서
구체 구현을 따라가도 버티는 데 큰 문제는 없을 거다.